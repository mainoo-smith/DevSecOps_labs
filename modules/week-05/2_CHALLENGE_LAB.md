üìÅ Week5/ChallengeLab.md

Title: End-to-End Secure Automation for a Production Microservice Stack

‚∏ª

üß™ Scenario

You‚Äôre now part of the DevSecOps team managing a growing microservices-based app (auth-service, notes-service, notification-service) hosted on ECS and Lambda.

You‚Äôre tasked with scripting a secure and portable daily automation routine that:
	1.	Checks health of all deployed services
	2.	Rotates old logs from each container
	3.	Generates and stores daily SBOM reports
	4.	Uploads all logs + reports to a secure S3 archive
	5.	Runs automatically via cron or CI/CD pipeline

You must ensure:
	‚Ä¢	Secrets are never exposed
	‚Ä¢	IAM roles are scoped correctly
	‚Ä¢	Logging avoids sensitive data
	‚Ä¢	CI jobs can run securely in both GitHub and GitLab environments

‚∏ª

üéØ Requirements

Task	Language	Tooling
ECS health & Lambda checks	Bash	AWS CLI
SBOM generation	Python	Trivy
Log rotation & archive	Bash + Python	gzip, boto3
Secrets loading	Bash	AWS SSM
GitOps integration	Both	GitHub Actions / GitLab CI


‚∏ª

üõ†Ô∏è Tasks

‚∏ª

‚úÖ 1. Create a unified Bash script scripts/automation/daily_ops.sh

This script should:
	‚Ä¢	Accept parameters (e.g. service name, region)
	‚Ä¢	Call other scripts to:
	‚Ä¢	Check service health (ECS + Lambda)
	‚Ä¢	Rotate & archive logs
	‚Ä¢	Generate SBOM and convert to HTML
	‚Ä¢	Upload artifacts to a specific devsecops-prod-logs S3 bucket
	‚Ä¢	Load secrets securely from SSM

The top of the script should look like:

#!/bin/bash
set -euo pipefail

ENV="${1:-prod}"
REGION="${2:-us-east-1}"

The script must log actions to a timestamped file in logs/ (not to stdout).

‚∏ª

‚úÖ 2. Write a Python script scripts/sbom/sbom_daily_report.py
	‚Ä¢	It should take an image name and output both .json and .html
	‚Ä¢	Store results in sbom/YYYY-MM-DD/ folders
	‚Ä¢	Upload to S3 automatically using boto3
	‚Ä¢	Send an optional summary to Slack (mock or print)

‚∏ª

‚úÖ 3. Secure .env file loading
	‚Ä¢	Create .env.example with:

AWS_REGION=
S3_ARCHIVE_BUCKET=
SLACK_WEBHOOK_URL=


	‚Ä¢	Bash and Python scripts should read this securely using dotenv or source .env
	‚Ä¢	In CI/CD: inject values via secrets manager (e.g., AWS_SECRET_ACCESS_KEY, SLACK_TOKEN)

‚∏ª

‚úÖ 4. GitLab or GitHub CI/CD Integration

Create a pipeline job to run your daily_ops.sh:

GitHub Actions example:

jobs:
  daily-automation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: |
          bash scripts/automation/daily_ops.sh prod us-east-1
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

GitLab CI example:

daily_ops:
  script:
    - bash scripts/automation/daily_ops.sh prod us-east-1
  only:
    - schedules


‚∏ª

üîê Security Requirements
	‚Ä¢	No secrets hardcoded in scripts
	‚Ä¢	Use IAM roles with least privilege (e.g., only allow upload to specific bucket)
	‚Ä¢	Avoid using eval, os.system() or unsanitized variables
	‚Ä¢	Set up .gitignore for .env, *.log, *.gz, and generated SBOM files

‚∏ª

üì¶ Output Artifacts (Required)

Artifact	Format	Notes
Health Check Logs	logs/ecs_health_YYYY-MM-DD.log	Plaintext, timestamped
SBOM JSON	sbom/YYYY-MM-DD/*.json	CycloneDX
SBOM HTML	sbom/YYYY-MM-DD/*.html	For human consumption
Uploaded logs	s3://devsecops-prod-logs/...	Auto-versioned
Archived app logs	*.gz	Auto-uploaded


‚∏ª

üí° Bonus Challenges
	‚Ä¢	Add Slack notifications using Python (requests.post) to alert if health check fails
	‚Ä¢	Encrypt log archives using AWS KMS before upload
	‚Ä¢	Support different environments (dev, qa, staging, prod) with config files

‚∏ª

‚úÖ Completion Criteria

You‚Äôve succeeded if:
	‚Ä¢	All scripts are functional, reusable, secure
	‚Ä¢	You can trigger the automation manually or via CI/CD
	‚Ä¢	Logs, SBOMs, and outputs are archived to S3
	‚Ä¢	Scripts run without needing root access
	‚Ä¢	No secrets are visible in logs or source control